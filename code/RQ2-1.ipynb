{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_issue_all = pd.read_csv('data/templates_repo.csv')\n",
    "df_issue = pd.read_csv('data/individual_issue_contents.csv')\n",
    "df_pr = pd.read_csv('data/individual_pr_contents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "candidate_labels = ['Summary', 'Type', 'Environment', 'Steps to reproduce', 'Expected/Actual behavior', 'Related issues', 'Additional context', 'Screenshot', 'Log', 'Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3143/3143 [8:02:58<00:00,  9.22s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Screenshot': 1333.5279111238196, 'Summary': 908.2945446299855, 'Related issues': 2207.674888310954, 'Additional context': 2332.6831313967705, 'Type': 2012.3376517063007, 'Steps to reproduce': 1842.7172833532095, 'Log': 1653.9070672942325, 'Environment': 1542.0549444421194, 'Severity': 1188.2451126333326, 'Expected/Actual behavior': 1233.4246628177352, 'repo': 'go-kratos_kratos', 'issue_file': 'question.md'}\n",
      "3143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# iterate over all issues and extract the labels and get the total score for each lable across all issues\n",
    "total_score = {}\n",
    "from tqdm import tqdm\n",
    "\n",
    "# get not null issues \n",
    "df_issue = df_issue[df_issue['body'].notnull()]\n",
    "df_issue = df_issue[df_issue['body'] != ' ']\n",
    "df_issue = df_issue[df_issue['body'] != '  ']\n",
    "\n",
    "df_issue = df_issue.iloc[int(len(df_issue)/2):]\n",
    "\n",
    "df_issue = df_issue.reset_index(drop=True)\n",
    "\n",
    "for i in tqdm(range(len(df_issue))):\n",
    "    results= classifier(df_issue.iloc[i]['body'], candidate_labels, multi_label=True)\n",
    "    for j in range(len(results['scores'])):\n",
    "        if results['labels'][j] not in total_score:\n",
    "            total_score[results['labels'][j]] = results['scores'][j]\n",
    "        else:\n",
    "            total_score[results['labels'][j]] += results['scores'][j]\n",
    "    total_score['repo'] = df_issue.iloc[i]['repo']\n",
    "    total_score['issue_file'] = df_issue.iloc[i]['issue_file']\n",
    "\n",
    "# print the total score for each label\n",
    "print(total_score)\n",
    "print(len(df_issue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Related issues', 'repo_name', 'issue_file', 'Additional context',\n",
       "       'Steps to reproduce', 'Expected/Actual behavior', 'Type', 'Screenshot',\n",
       "       'Log', 'Summary', 'Severity', 'Environment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_issue_scores = pd.read_csv('data/issue_labels.csv')\n",
    "df_issue_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related issues              18.581406\n",
      "Additional context          20.673790\n",
      "Steps to reproduce          17.256374\n",
      "Expected/Actual behavior     9.926884\n",
      "Type                         7.951364\n",
      "Screenshot                   5.914838\n",
      "Log                          4.712445\n",
      "Summary                      7.267429\n",
      "Severity                     3.486450\n",
      "Environment                  4.229020\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Exclude 'repo_name' and 'issue_file' from the columns to calculate average\n",
    "columns = ['Related issues', 'Additional context', 'Steps to reproduce', 'Expected/Actual behavior',\n",
    "           'Type', 'Screenshot', 'Log', 'Summary', 'Severity', 'Environment']\n",
    "\n",
    "# Calculate the average of each column\n",
    "average_labels = df_issue_scores[columns].mean() * 100\n",
    "\n",
    "# Print the average labels\n",
    "print(average_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greeting and Introduction                         4.499977\n",
      "Submission Guidelines and Formatting              5.142337\n",
      "Documentation and Testing                         5.678349\n",
      "Issue/Problem Description                        19.638139\n",
      "Proposed Solutions and Suggestions               12.175001\n",
      "Additional Context and Supporting Information    26.391812\n",
      "Related Issues and References                     8.811750\n",
      "Log/Debugging Information                         7.091120\n",
      "Severity and Impact Assessment                    4.727414\n",
      "Collaboration and Review                          5.844103\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories = [\n",
    "    \"Greeting and Introduction\",\n",
    "    \"Submission Guidelines and Formatting\",\n",
    "    \"Documentation and Testing\",\n",
    "    \"Issue/Problem Description\",\n",
    "    \"Proposed Solutions and Suggestions\",\n",
    "    \"Additional Context and Supporting Information\",\n",
    "    \"Related Issues and References\",\n",
    "    \"Log/Debugging Information\",\n",
    "    \"Severity and Impact Assessment\",\n",
    "    \"Collaboration and Review\"\n",
    "]\n",
    "\n",
    "df_issue_scores_2 = pd.read_csv('data/issue_labels-2.csv')\n",
    "\n",
    "# Calculate the average of each column\n",
    "average_labels_2 = df_issue_scores_2[categories].mean() * 100\n",
    "\n",
    "# Print the average labels\n",
    "print(average_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greeting and Introduction                         5.025954\n",
      "Submission Guidelines and Formatting              6.096200\n",
      "Documentation and Testing                        13.607567\n",
      "Issue/Problem Description                        15.162385\n",
      "Proposed Solutions and Suggestions                9.603613\n",
      "Additional Context and Supporting Information    23.828308\n",
      "Related Issues and References                     9.001904\n",
      "Log/Debugging Information                         4.671558\n",
      "Severity and Impact Assessment                    4.670882\n",
      "Collaboration and Review                          8.331628\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories = [\n",
    "    \"Greeting and Introduction\",\n",
    "    \"Submission Guidelines and Formatting\",\n",
    "    \"Documentation and Testing\",\n",
    "    \"Issue/Problem Description\",\n",
    "    \"Proposed Solutions and Suggestions\",\n",
    "    \"Additional Context and Supporting Information\",\n",
    "    \"Related Issues and References\",\n",
    "    \"Log/Debugging Information\",\n",
    "    \"Severity and Impact Assessment\",\n",
    "    \"Collaboration and Review\"\n",
    "]\n",
    "\n",
    "df_pr_scores_2 = pd.read_csv('data/pr_labels-2.csv')\n",
    "\n",
    "# Calculate the average of each column\n",
    "average_pr_labels_2 = df_pr_scores_2[categories].mean() * 100\n",
    "\n",
    "# Print the average labels\n",
    "print(average_pr_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Summary', 'repo_name', 'pr_file', 'Additional context',\n",
       "       'Steps to reproduce', 'Related issues', 'Screenshot',\n",
       "       'Expected/Actual behavior', 'Type', 'Environment', 'Log', 'Severity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_pr_scores = pd.read_csv('data/pr_labels.csv')\n",
    "df_pr_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related issues              18.350389\n",
      "Additional context          24.482078\n",
      "Steps to reproduce          12.812169\n",
      "Expected/Actual behavior     6.590542\n",
      "Type                         8.913374\n",
      "Screenshot                   5.695020\n",
      "Log                          4.750893\n",
      "Summary                     11.551874\n",
      "Severity                     3.996439\n",
      "Environment                  2.857222\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Exclude 'repo_name' and 'issue_file' from the columns to calculate average\n",
    "columns = ['Related issues', 'Additional context', 'Steps to reproduce', 'Expected/Actual behavior',\n",
    "           'Type', 'Screenshot', 'Log', 'Summary', 'Severity', 'Environment']\n",
    "\n",
    "# Calculate the average of each column\n",
    "average_labels = df_pr_scores[columns].mean() * 100\n",
    "\n",
    "# Print the average labels\n",
    "print(average_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write as csv file per repo \n",
    "df = pd.DataFrame.from_dict(total_score, orient='index')\n",
    "# put as repo name as colum name and each label as column \n",
    "df = df.reset_index(drop=True)\n",
    "df = df.rename(columns={'repo': 'repo_name'})\n",
    "df.to_csv('data/issue_labels-part-2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issue_labels_1 = pd.read_csv('data/issue_labels.csv')\n",
    "df_issue_labels_2 = pd.read_csv('data/issue_labels-2.csv')\n",
    "\n",
    "# combine the two dataframes and ignore the same columns \n",
    "df_issue_labels = pd.concat([df_issue_labels_1, df_issue_labels_2], axis=1, join='inner')\n",
    "\n",
    "df_issue_labels.to_csv('data/issue_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr_labels_1 = pd.read_csv('data/pr_labels.csv')\n",
    "df_pr_labels_2 = pd.read_csv('data/pr_labels-2.csv')\n",
    "\n",
    "df_pr_labels = pd.concat([df_pr_labels_1, df_pr_labels_2], axis=1, join='inner')\n",
    "df_pr_labels.to_csv('data/pr_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
